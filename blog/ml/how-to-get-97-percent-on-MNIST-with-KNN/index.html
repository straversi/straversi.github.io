<!DOCTYPE html><html><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><title>How to Get 97% on MNIST with KNN • steven.codes</title><meta name="description" content="Get the data"><link href="https://fonts.googleapis.com/css?family=Roboto+Mono:300,400,700" rel="stylesheet"><link href="https://fonts.googleapis.com/css?family=Bitter:400,700" rel="stylesheet"> <!--<link rel="stylesheet" href="/blog/css/main.css"> --><link rel="stylesheet" href="/blog/css/main.css?v=2.0"><link rel="canonical" href="http://steven.codes/blog/ml/how-to-get-97-percent-on-MNIST-with-KNN/"><link rel="alternate" type="application/rss+xml" title="steven.codes" href="http://steven.codes/blog/feed.xml"></head><body><div class="page-content"><header class="new-header"><div class="wrapper"> <a class="site-title" href="/blog/">steven.codes<span>/blog</span></a></div></header><div class="wrapper"><article class="post" itemscope itemtype="http://schema.org/BlogPosting"><header class="post-header"><h1 class="post-title" itemprop="name headline">How to Get 97% on MNIST with KNN</h1><p class="post-meta"> <time datetime="2016-05-07T04:53:36-07:00" itemprop="datePublished">May 7, 2016</time> <!-- <br>Read in about 7 minutes --></p></header><div class="post-content" itemprop="articleBody"><h2 id="get-the-data">Get the data</h2><p>This <a href="https://www.kaggle.com/c/digit-recognizer/data">Kaggle competition</a> is the source of my training data and test data. I also used it to calculate the final test score.</p><p>Numpy’s <a href="http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.genfromtxt.html#numpy-genfromtxt">genfromtxt</a> function is an easy way to get the .csv data into a matrix:</p><figure class="highlight"><figcaption>python</figcaption><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s">&#39;data/train.csv&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s">&#39;,&#39;</span><span class="p">,</span>
                  <span class="n">skip_header</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="s">&#39;uint8&#39;</span><span class="p">))</span></code></pre></figure><p>However, it’s slow, especially if you’ll be rerunning your program and reloading the data a lot. I recommend serializing the numpy matrix with the <code>pickle</code> module after the first load, and loading the saved pickle object on all subsequent runs of your program.</p><figure class="highlight"><figcaption>python</figcaption><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">pickle</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">&#39;data/train_points.p&#39;</span><span class="p">,</span> <span class="s">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
<span class="o">...</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">&#39;data/train_points.p&#39;</span><span class="p">,</span> <span class="s">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span></code></pre></figure><h2 id="write-knn">Write KNN</h2><p>KNN is fun to me because it trains in order 0 (zero) time. Here’s the setup for the actual implementation:</p><figure class="highlight"><figcaption>python</figcaption><pre><code class="language-python" data-lang="python"><span class="k">class</span> <span class="nc">KNN</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span></code></pre></figure><p>Predictions are where we start worrying about time. We’ll worry about that later. For now, let’s implement our own vanilla K-nearest-neighbors classifier. In the predict step, KNN needs to take a test point and find the closest sample to it in our training set. We’ll use the euclidian metric to assign distances between points, for ease.</p><p>Take the difference between all of the data and the incoming sample point at once with numpy’s element-wise subtraction: <code>differences = self.data - sample</code>. Then, to complete the distance calculation, take a row-wise inner product between <code>differences</code> and itself. Numpy’s <a href="http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.einsum.html#numpy-einsum">einsum</a> provides a fast execution. Lastly, get the <code>k</code> smallest distances and their corresponding label values. Here’s the final implementation:</p><figure class="highlight"><figcaption>python</figcaption><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">mode</span>
<span class="k">class</span> <span class="nc">KNN</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sample</span><span class="p">):</span>
      <span class="n">differences</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">data</span> <span class="o">-</span> <span class="n">sample</span><span class="p">)</span>
      <span class="n">distances</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s">&#39;ij, ij-&gt;i&#39;</span><span class="p">,</span> <span class="n">differences</span><span class="p">,</span> <span class="n">differences</span><span class="p">)</span>
      <span class="n">nearest</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">distances</span><span class="p">)[:</span><span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">]]</span>
      <span class="k">return</span> <span class="n">mode</span><span class="p">(</span><span class="n">nearest</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span></code></pre></figure><h2 id="improve-with-pca">Improve with PCA</h2><p>Our KNN currently considers all 784 features for each image when making its decisions. What if it doesn’t need that many? It’s possible that a lot of those features don’t <em>really</em> affect our predictions that much. Or worse, KNN could be considering feature anomalies that are unique to our training data, resulting in overfitting. One way to deal with this is by removing features that aren’t contributing much. Taking this concept further, <em>better</em> features, made up of linear combinations of the original features could be discovered. The original features are referred to as “axis aligned”, because our data is plotted against these feature axes. By finding better non-axis-aligned features, a new coordinate system for our data can be created composed of axes that run in more important directions (that is, the training data has higher variance along these axes). For a visual explanation, consider the following picture of a group of data:</p><p><img src="/blog/assets/posts/ml/how-to-get-97-percent-on-MNIST-with-KNN/gaussian.png " style="width:100%;max-width:400px;" /></p><p>If the data is fit to a Gaussian distribution, one can see that there are two eigenvectors which, if used as a basis when plotting the data, could provide a much higher variance among the data than our <script type="math/tex">x_1</script> and <script type="math/tex">x_2</script> axes. In other words, these two directions <script type="math/tex">x_a</script> and <script type="math/tex">x_b</script> tell us <em>more</em> about the data than <script type="math/tex">x_1</script> and <script type="math/tex">x_2</script>. Finding <script type="math/tex">x_a</script> and <script type="math/tex">x_b</script> and plotting our data in a new coordinate system based on these axes is called Principal Components Analysis (PCA).</p><p>Continuing from this idea of finding the eigenvectors that best describe our data, let’s talk math. Let <script type="math/tex">X</script> be a design matrix, <script type="math/tex">nxd</script>. If we assume <script type="math/tex">X</script> is centered, then its covariance matrix is <script type="math/tex">X^TX/(n-1)</script>, which can be decomposed as <script type="math/tex">V L V^T</script>, where <script type="math/tex">L</script> is a diagonal matrix composed of decreasing eigenvalues of the covariance matrix. <script type="math/tex">V</script> is made of their corresponding eigenvectors. The first <script type="math/tex">k</script> eigenvectors are the first <script type="math/tex">k</script> most important directions when it comes to our data. If we take <script type="math/tex">XV</script>, we get the projection of <script type="math/tex">X</script> onto <script type="math/tex">V</script>, placing the data in <script type="math/tex">X</script> into a basis that maximizes the variance of that data. The new and improved data is now composed of better, linearly uncorrelated variables that we call <em>principal components</em>.</p><p>Now, computing <script type="math/tex">X^TX</script> is not cheap: it takes <script type="math/tex">O(nd^2)</script> time. Luckily, to the rescue comes the Singular Value Decomposition (SVD). SVD can break our <script type="math/tex">nxd</script> design matrix into <script type="math/tex">X = UDV^T</script>, where <script type="math/tex">U</script> is composed of vertical left singular vectors of <script type="math/tex">X</script>, which are all orthogonal to each other. Similarly, the rows of <script type="math/tex">V</script> are the right singular vectors of <script type="math/tex">X</script>. <script type="math/tex">D</script> is diagonal, and its entries are the nonnegative singular values of <script type="math/tex">X</script>. At any rate, observe that the covariance matrix of <script type="math/tex">X</script> is estimated by <script type="math/tex">X^TX/(n-1)</script> <script type="math/tex">=VDU^TUDV^T/(n-1)</script> <script type="math/tex">=VD^2V^T/(n-1)</script>. The principal components are given by <script type="math/tex">XV = UDV^TV = UD</script>. Taking the first <script type="math/tex">k</script> columns of <script type="math/tex">U</script> and the first <script type="math/tex">k</script> entries of <script type="math/tex">S</script> gives us <script type="math/tex">U_kD_k</script>, the estimation of <script type="math/tex">XV</script> using only the first <script type="math/tex">k</script> principal components. In the end, we can find the <script type="math/tex">k</script> greatest singular values and their corresponding vectors in <script type="math/tex">O(ndk)</script> time. If <script type="math/tex">k</script> is chosen to be something like 40, then that’s a big time saving from 784 original dimensions.</p><p>I used numpy’s linalg package to solve the SVD of the design matrix. Here’s my function for using the SVD to find the PCA of the data (don’t forget to center the data).</p><figure class="highlight"><figcaption>python</figcaption><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">svd_pca</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">k</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Reduce DATA using its K principal components.&quot;&quot;&quot;</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&quot;float64&quot;</span><span class="p">)</span>
    <span class="n">data</span> <span class="o">-=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">U</span><span class="p">[:,:</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">S</span><span class="p">)[:</span><span class="n">k</span><span class="p">,:</span><span class="n">k</span><span class="p">])</span></code></pre></figure><p>Reducing the dimensionality of the MNIST data with PCA before running KNN can save both time and accuracy. Lower dimensions means less calculations and potentially less overfitting.</p><h2 id="cross-validation">Cross Validation</h2><p>Now the data can be preprocessed from an original dimension of 784 to some <script type="math/tex">k</script> « 784. There are two last questions: How many nearest-neighbors should we use in KNN? And how many dimensions should we reduce our data to through PCA?</p><p>When in doubt, cross validate. I set up a two dimensional cross validation test, and plotted the results:</p><p><img src="/blog/assets/posts/ml/how-to-get-97-percent-on-MNIST-with-KNN/cross_validation.png " style="width:100%;max-width:400px;" /></p><p>On the vertical axis is accuracy obtained via cross validation. On the horizontal axes are <script type="math/tex">k</script> for KNN, ranging from 2 to 12, and <script type="math/tex">k</script> for PCA, ranging from 5 to 80. The heat map on the lower plane helps illustrate that the best accuracies were achieved around <script type="math/tex">k_{NN} = 6</script>, <script type="math/tex">k_{PCA} = 45</script>. So, these are the values I used to predict on the Kaggle test set. Kaggle scored the submission at just over 97%. Not bad for around 12 lines of code (and numpy’s SVD solver)! ◼</p><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script></div><div id="disqus_thread"></div><script> var disqus_config = function () { this.page.url = "http://steven.codes/blog/ml/how-to-get-97-percent-on-MNIST-with-KNN/"; this.page.identifier = "/ml/how-to-get-97-percent-on-MNIST-with-KNN/"; this.page.title = "How to Get 97% on MNIST with KNN"; }; (function() { /* DON'T EDIT BELOW THIS LINE */ var d = document, s = d.createElement('script'); s.src = '//stevencodes.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript></article></div><footer class="site-footer"><div class="wrapper"><h3 class="footer-heading"><a href="/blog/">steven.codes</a></h3><div class="footer-col-wrapper"><div class="footer-col footer-col-1"><ul class="contact-list"> <!--<li>Steven Traversi</li>--><li><a href="mailto:straversi@berkeley.edu">straversi@berkeley.edu</a></li><li> <a href="https://github.com/straversi"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg> </span><span class="username">straversi</span></a></li><li><a href="https://github.com/straversi/straversi.github.io/issues">See something wrong?</a></li></ul></div><div class="footer-col footer-col-2"><ul class="social-media-list"> <!--<li> <a href="https://twitter.com/StevenTraversi"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg> </span><span class="username">StevenTraversi</span></a></li>--></ul></div><div class="footer-col footer-col-3"><p>I post here about some of the things I'm working on.</p></div></div></div></footer></div></body></html>
